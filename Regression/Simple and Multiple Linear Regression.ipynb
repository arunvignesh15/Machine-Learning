{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple and Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a statistical model that examines the linear relationship between two (Simple Linear Regression ) \n",
    "or more (Multiple Linear Regression) variables — a dependent variable and independent variable(s). \n",
    "Linear relationship basically means that when one (or more) independent variables increases (or decreases), \n",
    "the dependent variable increases (or decreases) too\n",
    "\n",
    "The simplest of probabilistic models is the straight line model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " y = β0 + β1x + ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- y = Dependent variable\n",
    "- x = Independent variable\n",
    "- ϵ = random error component\n",
    "- β0 = intercept/ Constant\n",
    "- β1 = Coefficient/Slope of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear relationship can be positive (independent variable goes up, dependent variable goes up) or negative (independent variable goes up, dependent variable goes down).\n",
    "\n",
    "A relationship between variables Y and X is represented by this equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y`i = mX + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation, Y is the dependent variable or the variable we are trying to predict or estimate; X is the independent variable, the variable we are using to make predictions; m is the slope of the regression line it represent the effect X has on Y. In other words, if X increases by 1 unit, Y will increase by exactly m units. b is a constant, also known as the Y-intercept. If X equals 0, Y would be equal to b. This is not necessarily applicable in real life — we won’t always know the exact relationship between X and Y or have an exact linear relationship.\n",
    "\n",
    "In a SLR model, we build a model based on data — the slope and Y-intercept derive from the data; furthermore, we don’t need the relationship between X and Y to be exactly linear. SLR models also include the errors in the data (also known as residuals). I won’t go too much into it now, maybe in a later post, but residuals are basically the differences between the true value of Y and the predicted/estimated value of Y. It is important to note that in a linear regression, we are trying to predict a continuous variable. In a regression model, we are trying to minimize these errors by finding the “line of best fit” — the regression line from the errors would be minimal. \n",
    "\n",
    "\n",
    "We are trying to minimize the length of the black lines (or more accurately, the distance of the blue dots) from the red line — as close to zero as possible. It is related to (or equivalent to) minimizing the mean squared error (MSE) or the sum of squares of error (SSE), also called the “residual sum of squares.” (RSS)\n",
    "`\n",
    "\n",
    "##### Add- ons \n",
    "  [To estimate the optimal values of m and b, you use a method called Ordinary Least Squares (OLS). This method tries to find    the parameters that minimize the sum of the squared errors, that is the vertical distance between the predicted y values and   the actual y values. The difference is known as the error term.]\n",
    "\n",
    "  Obtaining best fit : We use LSM(least square method) which gives the vertical distance between data point and\n",
    "  the regression line.\n",
    "  \n",
    "  We first need\ta measure of how well (or poorly)the model fits\tthe\ttrainingdata. The most common performance measure of a regression model is the Root Mean Square Error (RMSE). Therefore, to train a Linear Regression model, you need to find the value of θ that minimizes the RMSE. In practice, it is simpler to minimize the Mean Square Error (MSE) than the RMSE, and it leads to the same result (because the value that minimizes a function also minimizes its square root).\n",
    "\n",
    "#### Multiple Linear Regression: \n",
    "\n",
    "Most Practical Applications of Machine Learning involve Multiple Features on which the Target Outcome depends upon. Similarly in Regression Analysis Problems, there are instances where the Target Outcome depends on numerous features. Multi-Variate Linear Regression is a possible solution to tackle such problems. \n",
    "\n",
    "##### Add- ons \n",
    "In most cases, we will have more than one independent variable — we’ll have multiple variables; it can be as little as two independent variables and up to hundreds (or theoretically even thousands) of variables. in those cases we will use a Multiple Linear Regression model (MLR). The regression equation is pretty much the same as the simple regression equation, just with more variables.\n",
    "\n",
    "Multiple Linear Regression attempts to model the Relationship between two or more features and a response by fitting a linear equation to observed data. The steps to perform multiple linear Regression are almost similar to that of simple linear Regression. The Difference Lies in the Evalution. We can use it to find out which factor has the highest impact on the predicted output and now different variable relate to each other.\n",
    "\n",
    "Here : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y= b0 + b1*x1 + b2*x2 + b3*x3 +…… bn*xn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y = Dependent variable and x1, x2, x3, …… xn = multiple independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Assumptions\n",
    "\n",
    "- Linear relationship\n",
    "- Multivariate normality\n",
    "- No or little multicollinearity\n",
    "- No auto-correlation\n",
    "- Homoscedasticity\n",
    "\n",
    "First, linear regression needs the relationship between the independent and dependent variables to be linear.  It is also important to check for outliers since linear regression is sensitive to outlier effects.  The linearity assumption can best be tested with scatter plots, the following two examples depict two cases, where no and little linearity is present.\n",
    "\n",
    "Secondly, the linear regression analysis requires all variables to be multivariate normal.  This assumption can best be checked with a histogram or a Q-Q-Plot.  Normality can be checked with a goodness of fit test, e.g., the Kolmogorov-Smirnov test.  When the data is not normally distributed a non-linear transformation (e.g., log-transformation) might fix this issue.\n",
    "\n",
    "Thirdly, linear regression assumes that there is little or no multicollinearity in the data.  Multicollinearity occurs when the independent variables are too highly correlated with each other.\n",
    "\n",
    "Multicollinearity may be tested with three central criteria:\n",
    "\n",
    " 1) Correlation matrix – when computing the matrix of Pearson’s Bivariate Correlation among all independent variables the correlation coefficients need to be smaller than 1.\n",
    "\n",
    " 2) Tolerance – the tolerance measures the influence of one independent variable on all other independent variables; the tolerance is calculated with an initial linear regression analysis.  Tolerance is defined as T = 1 – R² for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the data and with T < 0.01 there certainly is.\n",
    "\n",
    " 3) Variance Inflation Factor (VIF) – the variance inflation factor of the linear regression is defined as VIF = 1/T. With \n",
    " VIF > 10 there is an indication that multicollinearity may be present; with VIF > 100 there is certainly multicollinearity among the variables.\n",
    " \n",
    "If multicollinearity is found in the data, centering the data (that is deducting the mean of the variable from each score) might help to solve the problem.  However, the simplest way to address the problem is to remove independent variables with high VIF values.\n",
    "\n",
    "Fourth, linear regression analysis requires that there is little or no autocorrelation in the data.  Autocorrelation occurs when the residuals are not independent from each other.  For instance, this typically occurs in stock prices, where the price is not independent from the previous price.\n",
    "\n",
    "Scatterplot allows you to check for autocorrelations, you can test the linear regression model for autocorrelation with the Durbin-Watson test.  Durbin-Watson’s d tests the null hypothesis that the residuals are not linearly auto-correlated.  While d can assume values between 0 and 4, values around 2 indicate no autocorrelation.  As a rule of thumb values of 1.5 < d < 2.5 show that there is no auto-correlation in the data. However, the Durbin-Watson test only analyses linear autocorrelation and only between direct neighbors, which are first order effects.\n",
    "\n",
    "The last assumption of the linear regression analysis is homoscedasticity.  The scatter plot is good way to check whether the data are homoscedastic (meaning the residuals are equal across the regression line).  The following scatter plots show examples of data that are not homoscedastic (i.e., heteroscedastic):\n",
    "\n",
    "The Goldfeld-Quandt Test can also be used to test for heteroscedasticity.  The test splits the data into two groups and tests to see if the variances of the residuals are similar across the groups.  If homoscedasticity is present, a non-linear correction might fix the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Involved in any Multiple Linear Regression Model\n",
    "\n",
    "Step #1: Data Pre Processing\n",
    "\n",
    "- Importing The Libraries.\n",
    "- Importing the Data Set.\n",
    "- Encoding the Categorical Data.\n",
    "- Avoiding the Dummy Variable Trap.\n",
    "- Splitting the Data set into Training Set and Test Set.\n",
    "\n",
    "Step #2: Fitting Multiple Linear Regression to the Training set\n",
    "\n",
    "Step #3: Predicting the Test set results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Template\n",
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv(r'C:\\Users\\ArunVignesh\\Desktop\\Machine Learning A-Z\\Part 2 - Regression\\Section 5 - Multiple Linear Regression\\Multiple_Linear_Regression\\50_Startups.csv')\n",
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
